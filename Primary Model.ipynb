{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Primary Model.ipynb","provenance":[],"mount_file_id":"1rmNDmx_A4mV1981MzK_1f7NfgpDhDGkJ","authorship_tag":"ABX9TyMtYlJmpFAOMsCsw56mmlWM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import transforms\n","import matplotlib.pyplot as plt"],"metadata":{"id":"dSXjUye1GLfz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Data Loader**"],"metadata":{"id":"-waQMa1O6N1T"}},{"cell_type":"code","source":["img_size = 256\n","batch_size = 16\n","train_dir = ''"],"metadata":{"id":"Yi3ZYkoo6RNX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose([transforms.CenterCrop(256),transforms.ToTensor()])\n","train_data = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"UwMjdk4A8sC3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Generator**"],"metadata":{"id":"qsZNpkyErl9Z"}},{"cell_type":"code","source":["class resBlock(nn.Moudle):\n","  def _init_(self):\n","    super(resBlock, self)._init_()\n","    self.conv_1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","    self.conv_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","    self.norm_1 = nn.BatchNorm2d(256)\n","    self.norm_2 = nn.BatchNorm2d(256)\n","  def forward(self, x):\n","    old = x\n","    x = F.relu(self.norm_1(self.conv_1(x)))\n","    x = self.norm_2(self.conv_2)\n","    return old + x"],"metadata":{"id":"RBBp9QEGGD8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLAPmCiFrcK9"},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self):\n","      super(Generator, self).__init__()\n","\n","      # first block\n","      self.conv_1 = nn.Conv2d(3, 64, 7, stride=1, padding=3)\n","      self.norm_1 = nn.BatchNorm2d(64)\n","      \n","      # down-convolution #\n","      self.conv_2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n","      self.conv_3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n","      self.norm_2 = nn.BatchNorm2d(128)\n","      \n","      self.conv_4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n","      self.conv_5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n","      self.norm_3 = nn.BatchNorm2d(256)\n","      \n","      # residual blocks #\n","      resBlock_list = []\n","      for i in range(8):\n","        resBlock_list.append(resBlock())\n","      self.resNetwork = nn.Sequential(*resBlock_list)\n","      \n","      # up-convolution #\n","      self.conv_6 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n","      self.conv_7 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=1)\n","      self.norm_4 = nn.BatchNorm2d(128)\n","\n","      self.conv_8 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n","      self.conv_9 = nn.ConvTranspose2d(64, 64, kernel_size=3, stride=1, padding=1)\n","      self.norm_5 = nn.BatchNorm2d(64)\n","      \n","      self.conv_10 = nn.Conv2d(64, 3, kernel_size=7, stride=1, padding=3)\n","\n","    def forward(self, x):\n","      # first block\n","      x = F.relu(self.norm_1(self.conv_1(x)))\n","      # down-convolution\n","      x = F.relu(self.norm_2(self.conv_3(self.conv_2(x))))\n","      x = F.relu(self.norm_3(self.conv_5(self.conv_4(x))))\n","      # residual blocks\n","      x = self.resNetwork(x)\n","      # up convolution\n","      x = F.relu(self.norm_4(self.conv_7(self.conv_6(x))))\n","      x = F.relu(self.norm_5(self.conv_9(self.conv_8(x))))\n","      # last block\n","      x = self.conv_10(x)\n","\n","      #x = sigmoid(x)\n","\n","      return x"]},{"cell_type":"markdown","source":["**Discriminator**"],"metadata":{"id":"9x6mi1PHrpdP"}},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","  def __init__(self):\n","     super(Discriminator, self).__init__()\n","     self.conv_1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1)\n","      \n","     self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2)\n","     self.conv_3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1)\n","     self.norm_1 = nn.BatchNorm2d(128)\n","      \n","     self.conv_4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2)\n","     self.conv_5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1)\n","     self.norm_2 = nn.BatchNorm2d(256)\n","    \n","     self.conv_6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1)\n","     self.norm_3 = nn.BatchNorm2d(256)\n","    \n","     self.conv_7 = nn.Conv2d(in_channels=256, out_channels=1, kernel_size=3, stride=1)\n","\n","  def forward(self, x):\n","    x = F.leaky_relu(self.conv_1(x))\n","    x = F.leaky_relu(self.norm_1(self.conv_3(F.leaky_relu(self.conv_2(x)))))\n","    x = F.leaky_relu(self.norm_2(self.conv_5(F.leaky_relu(self.conv_4(x)))))\n","    x = F.leaky_relu(self.norm_3(self.conv_6(x)))\n","    x = self.conv_7(x)\n","    #x = sigmoid(x)\n","    \n","    return x"],"metadata":{"id":"G3bxnXUDrpLv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**VGG16**"],"metadata":{"id":"UcLK6q7sh5tW"}},{"cell_type":"code","source":["# save pretrained vgg16 model to drive\n","from torchvision.models import vgg16\n","vgg16_path = '/content/drive/MyDrive/Colab Notebooks/APS360/Project/VGG16/vgg16_trained.pth'\n","vgg16_model = vgg16(pretrained=True)\n","torch.save(vgg16_model.state_dict(),vgg16_path)"],"metadata":{"id":"2Gylq_QkaQYv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load saved vgg16 model\n","from torchvision.models import vgg16\n","vgg16_path = '/content/drive/MyDrive/Colab Notebooks/APS360/Project/VGG16/vgg16_trained.pth'\n","pretrained_vgg = torch.load(vgg16_path)\n","vgg16_model = vgg16(pretrained=False)\n","vgg16_model.load_state_dict(pretrained_vgg)\n","#print(vgg16_model)"],"metadata":{"id":"3lWBokNG9IWd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vgg_features = vgg16_model.features[:24]\n","print(vgg_features)\n","# freeze the pretrained weights\n","for param in vgg_features.parameters():\n","  param.require_grad = False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vVArf94E8kNG","executionInfo":{"status":"ok","timestamp":1646767378889,"user_tz":300,"elapsed":162,"user":{"displayName":"CHENGYI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02936697074509049011"}},"outputId":"7f196f8f-d204-4b5a-c34c-77eff9b8cbbe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (1): ReLU(inplace=True)\n","  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (3): ReLU(inplace=True)\n","  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (6): ReLU(inplace=True)\n","  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (8): ReLU(inplace=True)\n","  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (11): ReLU(inplace=True)\n","  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (13): ReLU(inplace=True)\n","  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (15): ReLU(inplace=True)\n","  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (18): ReLU(inplace=True)\n","  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (20): ReLU(inplace=True)\n","  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (22): ReLU(inplace=True)\n","  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",")\n"]}]},{"cell_type":"markdown","source":["**Content Loss**"],"metadata":{"id":"9ZUgriWoPpJD"}},{"cell_type":"code","source":["def contentLoss(generate_img,real_img):\n","  loss = nn.L1Loss()\n","  return loss(vgg_features(generate_img),vgg_features(real_img))"],"metadata":{"id":"qF9EWnk4Pokz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Adversarial Loss**"],"metadata":{"id":"y_ekuEtAXh_6"}},{"cell_type":"code","source":["def advLoss(d_cartoon,d_removeEdge,d_generate):\n","  loss = nn.BCEWithLogitsLoss()\n","  print(d_cartoon.shape,d_removeEdge,d_generate)\n","  true_label = torch.ones(d_cartoon.shape)\n","  fake_label = torch.zeros(d_generate.shape)\n","  cartoon_loss = loss(d_cartoon,true_label)\n","  removeEdge_loss = loss(d_removeEdge,fake_label)\n","  generate_loss = loss(d_generate,fake_label)\n","  return cartoon_loss+removeEdge_loss+generate_loss"],"metadata":{"id":"EWuUauiWXpKt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Train**"],"metadata":{"id":"XMoLnl6FiHxo"}},{"cell_type":"code","source":["G_path = '/content/drive/MyDrive/Colab Notebooks/APS360/Project/Generator'\n","D_path = '/content/drive/MyDrive/Colab Notebooks/APS360/Project/Discriminator'"],"metadata":{"id":"HtAlOejKGWN5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr = 1e-4\n","epochs = 20"],"metadata":{"id":"9BazEl778u_f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(lr,epochs,batch_size):\n","  G = Generator()\n","  D = Discriminator()\n","  g_optimizer = optim.Adam(G.parameters(),lr=lr,betas=(0.5,0.999))\n","  d_optimizer = optim.Adam(D.parameters(),lr=lr,betas=(0.5,0.999))\n","  D_loss,G_loss = [],[]\n","  outputs = []\n","  test_img = \n","  for i in range(epochs):\n","    for j,((cartoon_img,_),(removeEdge_img,_),(real_img,_)) in enumerate(zip(cartoon_loader,\\\n","                                          smoothEdge_loader,real_loader)):\n","      # train discriminator\n","      generate_img = G(real_img)\n","      # advLoss components\n","      cartoon_eval = D(cartoon_img)\n","      removeEdge_eval = D(removeEdge_img)\n","      generate_eval = D(generate_img)\n","\n","      adv_loss = advLoss(cartoon_eval,removeEdge_eval,generate_eval)\n","      adv_loss.backward()\n","\n","      d_optimizer.step()\n","      d_optimizer.zero_grad()\n","\n","      # train generator\n","      content_loss = adv_loss + 10*contentLoss(generate_img,real_img)\n","      content_loss.backward()\n","\n","      g_optimizer.step()\n","      g_optimizer.zero_grad()\n","\n","    D_loss.append(adv_loss)\n","    G_loss.append(content_loss)\n","    outputs.append((i,real_img,generate_img),)\n","    print(\"Epoch: \",i,\" adv_loss: \",adv_loss,\" content_loss: \",content_loss)\n","  if (i+1) % 10==0:\n","    torch.save(G.state_dict(),G_path+'/G_epoch{i}.pth')\n","    torch.save(D.state_dict(),D_path+'/D_epoch{i}.pth')\n"," return outputs "],"metadata":{"id":"Epa_SHQYjbjR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs = train(lr,epochs,batch_size)"],"metadata":{"id":"5JZ5aw3U8BoQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot outputs\n","for k in range(0, max_epochs, 5):\n","    plt.figure(figsize=(9, 2))\n","    imgs = outputs[k][1].detach().numpy()\n","    recon = outputs[k][2].detach().numpy()\n","    for i, item in enumerate(imgs):\n","        if i >= 9: break\n","        plt.subplot(2, 9, i+1)\n","        plt.imshow(item[0])\n","        \n","    for i, item in enumerate(recon):\n","        if i >= 9: break\n","        plt.subplot(2, 9, 9+i+1)\n","        plt.imshow(item[0])"],"metadata":{"id":"7IYN2KpdHX3M"},"execution_count":null,"outputs":[]}]}